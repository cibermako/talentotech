{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAREA 1: Métodos de aprendizaje conjunto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preguntas conceptuales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. ¿Si un algoritmo `gradient boosting ensemble` muestra `sobreajuste`, ¿debería aumentar o disminuir la tasa de aprendizaje? Argumente su respuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta:**\n",
    "\n",
    "Se debería disminuir la tasa de aprendizaje.  El Gradient Boosting,  construye modelos secuenciales donde cada nuevo modelo corrige los errores del modelo anterior.  La tasa de aprendizaje afecta la velocidad con la que el modelo se ajusta a los datos. Si se disminuye la tasa de aprendizaje, cada nuevo  modelo afecta menos sobre la predicción final. por tanto el algoritmo hace correcciones más pequeñas en cada iteración es decir es mas gradual y controlado y disminuyendo el riesgo de sobreajuse.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Si ha entrenado 5 `modelos diferentes` de clasificación con exactamente los mismos datos de entrenamiento y todos ellos alcanzan una exactitud (`accuracy`) del $95\\%$, ¿existe alguna posibilidad de que pueda combinar estos modelos para obtener mejores resultados? Si es así, ¿cómo? Si no, ¿por qué?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta:**\n",
    "Sí es posible mejorar los resultados combinando los modelos, al ser metodos diferentes, los errores pueden ser diferentes y por tanto al combinarlos se complementan y se puede reducir el error total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Suponga que en el escenario de la pregunta `1.2` se combinan los `5` clasificadores base mediante el método de `voto mayoriatario duro`. Calcule el error del clasificador conjunto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Cargar un conjunto de datos (por ejemplo, el conjunto de datos Iris)\n",
    "data = load_iris()\n",
    "\n",
    "# Describir los datos\n",
    "X = data.data\n",
    "y = data.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del Voting Classifier: 1.00\n",
      "Error del clasificador conjunto: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Definir los cinco modelos base\n",
    "modelo1 = DecisionTreeClassifier(max_depth=3, random_state=42)  # Árbol de decisión\n",
    "modelo2 = KNeighborsClassifier(n_neighbors=5)  # K-Nearest Neighbors\n",
    "modelo3 = GaussianNB()  # Naive Bayes Gaussiano\n",
    "modelo4 = LogisticRegression(max_iter=1000, random_state=42)  # Regresión logística\n",
    "modelo5 = SVC(probability=True, kernel='linear', random_state=42)  # Support Vector Classifier con kernel lineal\n",
    "\n",
    "# Crear el Voting Classifier con voting='hard' (voto mayoritario duro)\n",
    "votador = VotingClassifier(estimators=[\n",
    "    ('decision_tree', modelo1),\n",
    "    ('svc', modelo2),\n",
    "    ('knn', modelo3),\n",
    "    ('naive_bayes', modelo4),\n",
    "    ('logistic_regression', modelo5)\n",
    "], voting='hard')\n",
    "\n",
    "# Ajustar el Voting Classifier a los datos de entrenamiento\n",
    "votador.fit(X_train, y_train)\n",
    "\n",
    "# Predecir las etiquetas para el conjunto de prueba\n",
    "predicciones_conjunto = votador.predict(X_test)\n",
    "\n",
    "# Calcular la exactitud del clasificador conjunto\n",
    "exactitud = accuracy_score(y_test, predicciones_conjunto)\n",
    "print(f'Exactitud del Voting Classifier: {exactitud:.2f}')\n",
    "\n",
    "# Calcular el error del clasificador conjunto\n",
    "error = 1 - exactitud\n",
    "print(f'Error del clasificador conjunto: {error:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Exactitud: 1.00, Error: 0.00\n",
      "KNN - Exactitud: 1.00, Error: 0.00\n",
      "Naive Bayes - Exactitud: 0.98, Error: 0.02\n",
      "Logistic Regression - Exactitud: 1.00, Error: 0.00\n",
      "SVM - Exactitud: 1.00, Error: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Evalua de forma individual \n",
    "modelos_base = {\n",
    "    'Decision Tree': modelo1,\n",
    "    'KNN': modelo2,\n",
    "    'Naive Bayes': modelo3,\n",
    "    'Logistic Regression': modelo4,\n",
    "    'SVM': modelo5\n",
    "}\n",
    "\n",
    "\n",
    "for nombre, modelo in modelos_base.items():\n",
    "    modelo.fit(X_train, y_train)\n",
    "    predicciones = modelo.predict(X_test)\n",
    "    exactitud = accuracy_score(y_test, predicciones)\n",
    "    error = 1 - exactitud\n",
    "    print(f'{nombre} - Exactitud: {exactitud:.2f}, Error: {error:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. ¿Si un algoritmo `Adaboost` muestra `subajuste`, ¿qué hiperparámetros debería ajustar y cómo? Argumente su respuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respuesta\n",
    "Significa que el modelo es demasiado simple para capturar la complejidad de los datos y, por lo tanto, no está obteniendo un rendimiento adecuado tanto en el conjunto de entrenamiento como en el de prueba. \n",
    "Para mejorar el rendimiento y reducir el subajuste, se pueden ajustar varios Hiperparametros:\n",
    "\n",
    "1. Aumentar el número de estimadores. puede ayudar a reducir el subajuste, ya que permite que el modelo aprenda más características de los datos. \n",
    "2. Reducir la tasa de aprendizaje.  puede ayudar a mejorar el rendimiento al hacer que el modelo se ajuste más lentamente y de manera más precisa a los datos. \n",
    "3. Incrementar la profundidad de los árboles, permite que cada clasificador débil sea más complejo, lo cual puede ayudar a reducir el subajuste. \n",
    "4. Cambiar el clasificador débil a uno más complejo, como un árbol de decisión de mayor profundidad o incluso otros modelos más sofisticados, puede ayudar a capturar mejor las características de los datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ejercicio(s) práctico(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos `mnist_784`, contiene 70.000 pequeñas imágenes de dígitos escritos a mano por estudiantes de secundaria y empleados de la Oficina del Censo de EE.UU. Es posible acceder a este dataset medinte las siguientes instrucciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10.8 s\n",
      "Wall time: 10.8 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9    ...  774  775  776  \\\n",
       "0        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "1        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "2        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "3        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "4        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "69995    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "69996    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "69997    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "69998    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "69999    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "\n",
       "       777  778  779  780  781  782  783  \n",
       "0        0    0    0    0    0    0    0  \n",
       "1        0    0    0    0    0    0    0  \n",
       "2        0    0    0    0    0    0    0  \n",
       "3        0    0    0    0    0    0    0  \n",
       "4        0    0    0    0    0    0    0  \n",
       "...    ...  ...  ...  ...  ...  ...  ...  \n",
       "69995    0    0    0    0    0    0    0  \n",
       "69996    0    0    0    0    0    0    0  \n",
       "69997    0    0    0    0    0    0    0  \n",
       "69998    0    0    0    0    0    0    0  \n",
       "69999    0    0    0    0    0    0    0  \n",
       "\n",
       "[70000 rows x 784 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', as_frame=False)\n",
    "X, y = mnist.data, mnist.target\n",
    "\n",
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La etiqueta de esta imagen es:  3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_digit(image_data):\n",
    "    image = image_data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "print(\"La etiqueta de esta imagen es: \", y[12])\n",
    "plot_digit(X[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.1 Escale todas las `784` variables con el método `min-max`; pero usando en máximo y el mínimo globales, en lugar de los de cada columna. Vrase el `ejercicio 0312` del capítulo 5, unidad 8 (`ensemble algorithms`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1    2    3    4    5    6    7    8    9    ...  774  775  776  777  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "\n",
      "   778  779  780  781  782  783  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 784 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Carga el dataset MNIST\n",
    "mnist = fetch_openml('mnist_784', as_frame=False)\n",
    "X, y = mnist.data, mnist.target\n",
    "\n",
    "# Convertimos el array a un DataFrame de pandas\n",
    "df = pd.DataFrame(X)\n",
    "\n",
    "# Crear una instancia del MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Ajustar y transformar el dataset\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Verifica el resultado\n",
    "print(df_normalized.head())\n",
    "\n",
    "# Selecciona una fila del DataFrame normalizado (por ejemplo, la primera fila)\n",
    "first_image = df_normalized.iloc[12].values\n",
    "\n",
    "# Plotea la imagen normalizada\n",
    "plot_digit(first_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.2 Divida los datos en un conjunto de entrenamiento y un conjunto de prueba (utilizar $60.000$ registros para el entrenamiento y $10.000$ para prueba). Luego, entrene los siguientes clasificadores optimizando los `hiperparámetros` más representativos (excepto los de regularización) mediante la `validación cruzada `:\n",
    "\n",
    "  - clasificador SVM.\n",
    "  - Regresión logística\n",
    "  - Naive Bayes\n",
    "  - K-vecinos\n",
    "  - Árbol de decisión\n",
    "\n",
    "  Compare los rendimientos de cada uno de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Carga el dataset MNIST\n",
    "mnist = fetch_openml('mnist_784', as_frame=False)\n",
    "X, y = mnist.data, mnist.target\n",
    "\n",
    "# Normaliza los datos\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Divide el dataset en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, train_size=60000, test_size=10000, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los clasificadores y los hiperparámetros a optimizar\n",
    "classifiers = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "    \"NaiveBayes\": GaussianNB(),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(),\n",
    "    \"SVM\": SVC(probability=True) \n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    \"LogisticRegression\": {\n",
    "        \"solver\": [\"lbfgs\", \"liblinear\"],\n",
    "        \"C\": [0.1, 1, 10], # Parametros de ajuse, de mayor (0.1) a menor regularización penaliza más los coeficientes grandes, lo que tiende a reducir el ajuste \n",
    "                           # del modelo a los datos de entrenamiento y puede mejorar su capacidad de generalización a nuevos datos.\n",
    "                           # (10) El modelo intentará ajustarse lo más posible a los datos de entrenamiento. \n",
    "        \"n_jobs\":[-1]      # Utiliza todos los núcleos disponibles\n",
    "    },\n",
    "    \"NaiveBayes\": {\n",
    "        # GaussianNB no tiene hyperparaemtros excepto var_smoothing el cual es para regularización\n",
    "    },\n",
    "    \"KNeighbors\": {\n",
    "        \"n_neighbors\": [3, 5, 7],   # número de vecinos más cercanosm, con menos vecinos puede capturar mejor la estructura local del espacio de datos, \n",
    "                                    # pero puede ser sensible al ruido y a las variaciones en los datos.\n",
    "                                    # con más vecinos, Proporciona una decisión más general y robusta, pero puede suavizar demasiado y perder la estructura local específica. \n",
    "        \"weights\": [\"uniform\", \"distance\"], # uniform: todos los vecinos tienen el mismo peso en la votación \n",
    "                                           #  distance: Los vecinos más cercanos tienen más influencia en la decisión.\n",
    "        \"n_jobs\":[-1]                # Utiliza todos los núcleos disponibles\n",
    "    },\n",
    "    \"DecisionTree\": {\n",
    "        \"criterion\": [\"gini\", \"entropy\"], # \"gini\": Utiliza el índice de Gini para medir la impureza de un nodo.\n",
    "                                          # \"entropy\": Utiliza la entropía para medir la impureza de un nodo.\n",
    "        \"splitter\": [\"best\", \"random\"],   # \"best\": Busca la mejor división posible en cada nodo.\n",
    "                                          # \"random\": Selecciona la mejor división entre un subconjunto aleatorio de características. \n",
    "        \"max_depth\": [None, 10, 20]       # None: No hay límite en la profundidad del árbol.\n",
    "                                          # 10: La profundidad máxima del árbol es 10 niveles.\n",
    "                                          # 20: La profundidad máxima del árbol es 20 niveles.  \n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"kernel\": [\"linear\", \"rbf\"], # linear\": Utiliza un kernel lineal.\n",
    "                                     #\"rbf\": Utiliza un kernel de función de base radial (kernel Gaussiano).\n",
    "        \"C\": [0.1, 1, 10],           # Controla la regularización. Valores altos de C reducen la regularización (más ajustado a los datos de entrenamiento), \n",
    "                                     # y valores bajos de C aumentan la regularización (mejor generalización).\n",
    "        \"gamma\": [\"scale\", \"auto\"]  # scale: gamma se calcula como  1/n -varizanza(X).   Puede llevar a valores altos de gama, \n",
    "                                     # que puede hacer el modelo mas susceptible a sobreajuste\n",
    "                                     # auto: se calcula como 1/n    n=Número de caracteristicas\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando LogisticRegression...\n",
      "Mejores Parámetros para LogisticRegression: {'C': 0.1, 'n_jobs': -1, 'solver': 'lbfgs'}\n",
      "Entrenando NaiveBayes...\n",
      "Entrenando KNeighbors...\n",
      "Mejores Parámetros para KNeighbors: {'n_jobs': -1, 'n_neighbors': 3, 'weights': 'distance'}\n",
      "Entrenando DecisionTree...\n",
      "Mejores Parámetros para DecisionTree: {'criterion': 'entropy', 'max_depth': None, 'splitter': 'best'}\n",
      "Entrenando SVM...\n",
      "Mejores Parámetros para SVM: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# Entrenar los clasificadores y optimizar los hiperparámetros mediante la validación cruzada\n",
    "best_estimators = {}\n",
    "for clf_name in classifiers:\n",
    "    print(f\"Entrenando {clf_name}...\")\n",
    "    if clf_name == \"NaiveBayes\":\n",
    "        best_estimators[clf_name] = classifiers[clf_name]\n",
    "        best_estimators[clf_name].fit(X_train, y_train)\n",
    "    else:\n",
    "        grid_search = GridSearchCV(classifiers[clf_name], param_grids[clf_name], cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_estimators[clf_name] = grid_search.best_estimator_\n",
    "        print(f\"Mejores Parámetros para {clf_name}: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression accuracy: 0.9225\n",
      "NaiveBayes accuracy: 0.5563\n",
      "KNeighbors accuracy: 0.9751\n",
      "DecisionTree accuracy: 0.8778\n",
      "SVM accuracy: 0.9836\n",
      "\n",
      "Rendimiento de los clasificadores:\n",
      "\n",
      "                    Accuracy\n",
      "LogisticRegression    0.9225\n",
      "NaiveBayes            0.5563\n",
      "KNeighbors            0.9751\n",
      "DecisionTree          0.8778\n",
      "SVM                   0.9836\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el rendimiento de los clasificadores en el conjunto de prueba\n",
    "results = {}\n",
    "for clf_name in best_estimators:\n",
    "    clf = best_estimators[clf_name]\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[clf_name] = accuracy\n",
    "    print(f\"{clf_name} accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Mostrar los resultados\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index', columns=['Accuracy'])\n",
    "print(\"\\nRendimiento de los clasificadores:\\n\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.3 Utilizando la combinación de hiperparámetroas más óptima para cada uno de los modelos anteriores, construya un clasificador por `voto mayoritario` tanto tipo `hard` como tipo `soft`. Elija el más `eficiente` de los dos métodos y compare su rendimientro contra el mejor clasificador undividual del numeral anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier (Hard) accuracy: 0.9695\n",
      "Voting Classifier (Soft) accuracy: 0.9708\n",
      "\n",
      "Mejor clasificador individual: SVM con una precisión de 0.9836\n",
      "\n",
      "Mejor clasificador por voto mayoritario: Soft Voting con una precisión de 0.9708\n",
      "\n",
      "El mejor clasificador individual (SVM) es mejor que el clasificador por voto mayoritario (Soft Voting).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Construir un clasificador por voto mayoritario\n",
    "voting_clf_hard = VotingClassifier(\n",
    "    estimators=[(clf_name, best_estimators[clf_name]) for clf_name in best_estimators],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "voting_clf_soft = VotingClassifier(\n",
    "    estimators=[(clf_name, best_estimators[clf_name]) for clf_name in best_estimators],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Entrenar los clasificadores por voto mayoritario\n",
    "voting_clf_hard.fit(X_train, y_train)\n",
    "voting_clf_soft.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el rendimiento de los clasificadores por voto mayoritario\n",
    "y_pred_hard = voting_clf_hard.predict(X_test)\n",
    "y_pred_soft = voting_clf_soft.predict(X_test)\n",
    "\n",
    "accuracy_hard = accuracy_score(y_test, y_pred_hard)\n",
    "accuracy_soft = accuracy_score(y_test, y_pred_soft)\n",
    "\n",
    "print(f\"Voting Classifier (Hard) accuracy: {accuracy_hard:.4f}\")\n",
    "print(f\"Voting Classifier (Soft) accuracy: {accuracy_soft:.4f}\")\n",
    "\n",
    "# Comparar el rendimiento de los clasificadores por voto mayoritario con el mejor clasificador individual\n",
    "best_individual_clf = max(results, key=results.get)\n",
    "best_individual_accuracy = results[best_individual_clf]\n",
    "\n",
    "print(f\"\\nMejor clasificador individual: {best_individual_clf} con una precisión de {best_individual_accuracy:.4f}\")\n",
    "\n",
    "if accuracy_soft > accuracy_hard:\n",
    "    best_voting_accuracy = accuracy_soft\n",
    "    best_voting_method = \"Soft\"\n",
    "else:\n",
    "    best_voting_accuracy = accuracy_hard\n",
    "    best_voting_method = \"Hard\"\n",
    "\n",
    "print(f\"\\nMejor clasificador por voto mayoritario: {best_voting_method} Voting con una precisión de {best_voting_accuracy:.4f}\")\n",
    "\n",
    "if best_voting_accuracy > best_individual_accuracy:\n",
    "    print(f\"\\nEl clasificador por voto mayoritario ({best_voting_method} Voting) es mejor que el mejor clasificador individual.\")\n",
    "else:\n",
    "    print(f\"\\nEl mejor clasificador individual ({best_individual_clf}) es mejor que el clasificador por voto mayoritario ({best_voting_method} Voting).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.4 Utilice el `mejor modelo individual` hallado en el numeral `2.2` y contruya un clasificador con el método `Bagging`. Compare su rendimiento con los modelos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Construir un clasificador con el método Bagging utilizando el mejor modelo individual\n",
    "best_individual_clf = max(results, key=results.get)\n",
    "best_individual_accuracy = results[best_individual_clf]\n",
    "best_model = best_estimators[best_individual_clf]\n",
    "\n",
    "bagging_clf = BaggingClassifier(estimator=best_model, n_estimators=50, random_state=42)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el rendimiento del clasificador Bagging en el conjunto de prueba\n",
    "y_pred_bagging = bagging_clf.predict(X_test)\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "\n",
    "print(f\"\\nBagging Classifier (base: {best_individual_clf}) accuracy: {accuracy_bagging:.4f}\")\n",
    "\n",
    "# Comparar el rendimiento del clasificador Bagging con el mejor clasificador individual y el clasificador por voto mayoritario\n",
    "if accuracy_bagging > best_individual_accuracy and accuracy_bagging > best_voting_accuracy:\n",
    "    print(f\"\\nEl clasificador Bagging es el mejor con una precisión de {accuracy_bagging:.4f}\")\n",
    "elif best_voting_accuracy > accuracy_bagging:\n",
    "    print(f\"\\nEl mejor clasificador es el clasificador por voto mayoritario ({best_voting_method} Voting) con una precisión de {best_voting_accuracy:.4f}\")\n",
    "else:\n",
    "    print(f\"\\nEl mejor clasificador es el mejor clasificador individual ({best_individual_clf}) con una precisión de {best_individual_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.5 Utilizando los hiperparámetros del mejor `árbol de decisión` construido en el numeral `2.2`, entrene un `bosque alretorio` y utilice la validación cruzada afinar solamente un hiperparámetro: `n_estimators`. Compare su rendimiento con los modelos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Tomando los mejores hiperparámetros del mejor árbol de decisión\n",
    "best_tree_params = {\n",
    "    \"criterion\": \"entropy\",  \n",
    "    \"splitter\": \"best\",      \n",
    "    \"max_depth\": None        \n",
    "}\n",
    "\n",
    "# Definir el clasificador RandomForest y los hiperparámetros a afinar\n",
    "rf_clf = RandomForestClassifier(\n",
    "    criterion=best_tree_params[\"criterion\"],\n",
    "    max_depth=best_tree_params[\"max_depth\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200]\n",
    "}\n",
    "\n",
    "# Afinar n_estimators mediante validación cruzada\n",
    "grid_search_rf = GridSearchCV(rf_clf, param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Obtener el mejor clasificador\n",
    "best_rf_clf = grid_search_rf.best_estimator_\n",
    "print(f\"Best params for RandomForest: {grid_search_rf.best_params_}\")\n",
    "\n",
    "# Evaluar el rendimiento del bosque aleatorio en el conjunto de prueba\n",
    "y_pred_rf = best_rf_clf.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"\\nRandom Forest Classifier accuracy: {accuracy_rf:.4f}\")\n",
    "\n",
    "# Comparar con el mejor clasificador individual y el mejor clasificador por voto mayoritario (asumiendo que ya tienes estos valores)\n",
    "best_individual_accuracy = 0.9710  # Ejemplo: SVM con precisión de 97.10%\n",
    "best_voting_accuracy = 0.9750      # Ejemplo: Soft Voting con precisión de 97.50%\n",
    "\n",
    "print(f\"\\nMejor clasificador individual accuracy: {best_individual_accuracy:.4f}\")\n",
    "print(f\"Mejor clasificador por voto mayoritario accuracy: {best_voting_accuracy:.4f}\")\n",
    "\n",
    "if accuracy_rf > best_individual_accuracy and accuracy_rf > best_voting_accuracy:\n",
    "    print(f\"\\nEl clasificador Random Forest es el mejor con una precisión de {accuracy_rf:.4f}\")\n",
    "elif best_voting_accuracy > accuracy_rf:\n",
    "    print(f\"\\nEl mejor clasificador es el clasificador por voto mayoritario con una precisión de {best_voting_accuracy:.4f}\")\n",
    "else:\n",
    "    print(f\"\\nEl mejor clasificador es el mejor clasificador individual con una precisión de {best_individual_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.6 Utilice el `mejor modelo individual` hallado en el numeral `2.2` y contruya un clasificador con el método `AdaBoost`y utilice la validación cruzada afinar solamente dos hiperparámetros: `n_estimators` y `learning_rate`. Compare su rendimiento con los modelos anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.7 Utilizando los hiperparámetros del mejor `árbol de decisión` construido en el numeral `2.2`, entrene un `GradientBoostingClassifier` y utilice la validación cruzada afinar solamente dos hiperparámetros: `n_estimators` y `learning_rate`. Compare su rendimiento con los modelos anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.8 Utilizando los hiperparámetros del mejor `árbol de decisión` construido en el numeral `2.2`, entrene un `XGBClassifier` y utilice la validación cruzada afinar solamente dos hiperparámetros: `n_estimators` y `learning_rate`. Compare su rendimiento con los modelos anteriores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.9 Elija `el mejor clasificador` de entre todos los entrenados previamente y argumente su respuesta."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SFC_V2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
